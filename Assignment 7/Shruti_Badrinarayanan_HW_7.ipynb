{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Iw4m_8PcG5M"
      },
      "source": [
        "# Homework 7\n",
        "## Deep Learning Technologies\n",
        "### Shruti Badrinarayanan - 016768141"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REpmL57lJUDN"
      },
      "source": [
        "## Step 1: Load the Wikipedia GLoVE Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip -d glove"
      ],
      "metadata": {
        "id": "tn0tJvYl07tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "glove_input_file = 'glove/glove.6B.50d.txt'\n",
        "word2vec_output_file = 'glove/glove.6B.50d.txt.word2vec'\n",
        "glove2word2vec(glove_input_file, word2vec_output_file)\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Load the converted model\n",
        "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l3I_-Jb1BNi",
        "outputId": "ec3afacb-76fe-47a8-abc7-d627bdf9e206"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-968fe8421a85>:5: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
            "  glove2word2vec(glove_input_file, word2vec_output_file)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW6FgFJVImAW"
      },
      "source": [
        "## Step 2: Show how similar are these words:\n",
        "\n",
        "                 Man and Woman\n",
        "\n",
        "                 Chair and Throne\n",
        "\n",
        "                 water and baby"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xB2-NCyHKF_4",
        "outputId": "e81f7b35-d0a8-4ad3-f3be-67bbdc9c78a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between man and woman: 0.8860337734222412\n",
            "Similarity between chair and throne: 0.27968090772628784\n",
            "Similarity between water and baby: 0.40810367465019226\n"
          ]
        }
      ],
      "source": [
        "# Check similarity between different pairs of words\n",
        "words_pairs = [('man', 'woman'), ('chair', 'throne'), ('water', 'baby')]\n",
        "for word1, word2 in words_pairs:\n",
        "    print(f\"Similarity between {word1} and {word2}: {model.similarity(word1, word2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnoCodFmKObk"
      },
      "source": [
        "## Step 3: Using these provide analogies for the following:\n",
        "\n",
        "             _____ is to King as Woman is to Man.\n",
        "\n",
        "             _____ is to Princess as Man is to Woman.\n",
        "\n",
        "             _____ is to a woman as a child is to an adult."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2dp43M-z6J5",
        "outputId": "0f696428-e501-4dc3-901e-7e572f0f485f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "queen is to king as woman is to man.\n",
            "prince is to princess as man is to woman.\n",
            "male is to adult as woman is to child.\n"
          ]
        }
      ],
      "source": [
        "# Define the analogy tasks\n",
        "analogy_tasks = [\n",
        "    ('king', 'man', 'woman'),\n",
        "    ('princess', 'woman', 'man'),\n",
        "    ('adult', 'child', 'woman')\n",
        "]\n",
        "\n",
        "# Solve each analogy\n",
        "for word1, word2, word3 in analogy_tasks:\n",
        "      # Predict the most similar word\n",
        "      predicted = model.most_similar(positive=[word1, word3], negative=[word2], topn=1)\n",
        "      print(f\"{predicted[0][0]} is to {word1} as {word3} is to {word2}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj9SnoJUKjh4"
      },
      "source": [
        "## Step 4: Apply Naive-Bayes Classifier on the Spam-Ham dataset shown in the demo."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSNrTUZauXVz",
        "outputId": "f55d90d1-f4a3-4744-a59f-48aa483962d2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/Deep Learning/Assignments/Assignment 7/spam.csv', encoding='latin-1')\n",
        "\n",
        "# Only keep the necessary columns and rename them for convenience\n",
        "data = data[['v1', 'v2']]\n",
        "data.columns = ['label', 'message']\n",
        "\n",
        "# Define a function for preprocessing text\n",
        "def preprocess_text(sms):\n",
        "    # Remove punctuation\n",
        "    sms = \"\".join([char for char in sms if char not in string.punctuation])\n",
        "    # Convert text to lowercase\n",
        "    sms = sms.lower()\n",
        "    # Remove stopwords\n",
        "    sms_tokens = word_tokenize(sms)\n",
        "    sms_clean = [word for word in sms_tokens if word not in stopwords.words('english')]\n",
        "    return \" \".join(sms_clean)\n",
        "\n",
        "# Apply preprocessing to the messages\n",
        "data['message_clean'] = data['message'].apply(preprocess_text)\n",
        "\n",
        "# Splitting the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['message_clean'], data['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature Extraction using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train a Naive Bayes Classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_train = nb_classifier.predict(X_train_tfidf)\n",
        "y_pred_test = nb_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "conf_matrix_train = confusion_matrix(y_train, y_pred_train)\n",
        "conf_matrix_test = confusion_matrix(y_test, y_pred_test)\n",
        "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
        "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "(conf_matrix_train, accuracy_train, conf_matrix_test, accuracy_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWPOuzV2tpqX",
        "outputId": "47aded50-7aab-4e18-bd9c-95eb023b4449"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[3860,    0],\n",
              "        [  80,  517]]),\n",
              " 0.9820507067534215,\n",
              " array([[965,   0],\n",
              "        [ 33, 117]]),\n",
              " 0.9704035874439462)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Set Results:\\n\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix_train)\n",
        "print(f\"Accuracy: {accuracy_train * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWakszQlvfoB",
        "outputId": "b91e2836-253f-4b30-b892-e30d538f9889"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Results:\n",
            "\n",
            "Confusion Matrix:\n",
            "[[3860    0]\n",
            " [  80  517]]\n",
            "Accuracy: 98.21%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTest Set Results:\\n\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix_test)\n",
        "print(f\"Accuracy: {accuracy_test * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNvyvl1gvgBr",
        "outputId": "ab2335a1-a96d-44fc-8ac8-4dd4b7e288de"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Set Results:\n",
            "\n",
            "Confusion Matrix:\n",
            "[[965   0]\n",
            " [ 33 117]]\n",
            "Accuracy: 97.04%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}